from pydub import AudioSegment
import librosa
import pydub
import contextlib
import wave
from os import listdir
import os
import audiosegment
import numpy as np
import matplotlib.pyplot as plt
import itertools
from sys import byteorder
from array import array
from struct import pack

#import pyaudio
from scipy import signal
from scipy.io import wavfile
import random
from skimage.measure import block_reduce
import time
from keras import models
from keras import layers




#import pyaudio
from skimage.transform import resize
#To find the duration of wave file in seconds



#Keras imports
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
from keras.models import model_from_json


import datetime
import scipy as sp
from scipy.io.wavfile import read
from scipy.io.wavfile import write     # Imported libaries such as numpy, scipy(read, write), matplotlib.pyplot
#get_ipython().magic('matplotlib inline')
import sys
import math


pat=['C:/Users/sarathkumar.h/Desktop/train_datatt','C:/Users/sarathkumar.h/Desktop/test_datatt','C:/Users/sarathkumar.h/Desktop/train_data/','C:/Users/sarathkumar.h/Desktop/test_data/','C:/Users/sarathkumar.h/Desktop/1sec_train','C:/Users/sarathkumar.h/Desktop/1sec_test']

num_classes=2

paths = [pat[0],pat[1]]
outname =[pat[2],pat[3]]
for l in range(len(paths)):
    fname =  [f for f in listdir(paths[l])] 
    print(fname)
    #outname = 'filtered.wav'
    cutOffFrequency = 400.0
    
    for h in range(len(fname)):
        def running_mean(x, windowSize):
            cumsum = np.cumsum(np.insert(x, 0, 0)) 
            return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize

        def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True):
            if sample_width == 1:
                dtype = np.uint8
            elif sample_width == 2:
                dtype = np.int16 
            else:
                raise ValueError("Only supports 8 and 16 bit audio formats.")
            channels = np.fromstring(raw_bytes, dtype=dtype)
            if interleaved:
                channels.shape = (n_frames, n_channels)
                channels = channels.T
            else:
                channels.shape = (n_channels, n_frames)
            return channels
        with contextlib.closing(wave.open(paths[l]+'/'+fname[h],'rb')) as spf:
            sampleRate = spf.getframerate()
            ampWidth = spf.getsampwidth()
            nChannels = spf.getnchannels()
            nFrames = spf.getnframes()
            signal = spf.readframes(nFrames*nChannels)
            spf.close()
            channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True)
            freqRatio = (cutOffFrequency/sampleRate)
            N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio)
            filtered = running_mean(channels[0], N).astype(channels.dtype)
            wav_file = wave.open(outname[l]+fname[h], "w")
            wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname()))
            wav_file.writeframes(filtered.tobytes('C'))
            wav_file.close()
            
            
def hh(s,e,file):
    frrames=[]
    so=audiosegment.from_file(file)
    with contextlib.closing(wave.open(file,'r')) as f:
        firames = f.getnframes()
        irate = f.getframerate()
        iduration = firames / float(irate)
        
    ss=s
    sec_ee=0
    lllpo=[]
    while sec_ee <= iduration:
        ee=ss+e
        lllpo.append([ss,ee])
        frrames.append(so[ss:ee])
        ss=ee
        drf=ee/1000
        sec_ee=drf
    return frrames,lllpo

        
        

            
imwidth = 50
imheight= 34

def findDuration(fname):
    with contextlib.closing(wave.open(fname,'r')) as f:
        frames = f.getnframes()
        rate = f.getframerate()
        sw   = f.getsampwidth()
        chan = f.getnchannels()
        duration = frames / float(rate)
        #print("File:", fname, "--->",frames, rate, sw, chan)
        return duration

def graph_spectrogram(wav_file, nfft=512, noverlap=511):
    #findDuration(wav_file)
    rate, data = wavfile.read(wav_file)
    #print("")
    fig,ax = plt.subplots(1)
    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)
    ax.axis('off')
    pxx, freqs, bins, im = ax.specgram(x=data, Fs=rate, noverlap=noverlap, NFFT=nfft)
    ax.axis('off')
    plt.rcParams['figure.figsize'] = [0.75,0.5]
    #fig.savefig('sp_xyz.png', dpi=300, frameon='false')
    fig.canvas.draw()
    size_inches  = fig.get_size_inches()
    dpi          = fig.get_dpi()
    width, height = fig.get_size_inches() * fig.get_dpi()
    #print(size_inches, dpi, width, height)
    mplimage = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
    #print("MPLImage Shape: ", np.shape(mplimage))
    imarray = np.reshape(mplimage, (int(height), int(width), 3))
    plt.close(fig)
    fig.clf()
    del ax
    del pxx
    del freqs
    del bins
    del im
    del data
    del fig
    del mplimage
    
    return imarray
    del imarray

    
def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])

def normalize_gray(array):
    return (array - array.min())/(array.max() - array.min())

def create_train(audio_dir):
    from PIL import Image
    file_names = [f for f in os.listdir(audio_dir) ]
    file_names.sort()
    train_list = []
    
   
      
    train_list = [x for x in file_names]

    
    y_train = np.zeros(len(train_list))
    x_train = np.zeros((len(train_list), imheight, imwidth))
    

    

    

    for i, f in enumerate(train_list):
        if 'music' in f:
            y_train[i] = int(0)
        elif 'other' in f:
            y_train[i] = int(1)
        
        else:
            print('no ivr or music wave')
        
        
        spectrogram   = graph_spectrogram( audio_dir + '/'+ f )
        graygram      = rgb2gray(spectrogram)
        shape         = (34,50)
        reshape       = resize(graygram,shape)
        normgram      = normalize_gray(reshape)
        
        
        
        
        redgram       = block_reduce(normgram ,block_size=(1,1) ,func = np.mean)
        x_train[i,:,:] = redgram
        #if len(train_list)-5 == i:
         #   img=Image.fromarray(normgram,'RGB')
          #  img.save(r'C:\Users\sarathkumar.h\Desktop\mp33\afr.png')
            
        del spectrogram 
        del graygram
        del shape
        del reshape
        del normgram 
        del redgram
        print("Progress Training Data: {:2.1%}".format(float(i) / len(train_list)), end="\r")
        
    return x_train, y_train

def create_test(audio_dir):
    from PIL import Image
    file_names = [f for f in os.listdir(audio_dir) ]
    file_names.sort()
    test_list = []
    
   
      
    test_list = [x for x in file_names]

    
    y_test = np.zeros(len(test_list))
    x_test = np.zeros((len(test_list), imheight, imwidth))
    

    

    

    for i, f in enumerate(test_list):
        if 'music' in f:
            y_test[i] = int(0)
        elif 'other' in f:
            y_test[i] = int(1)
        
        else:
            print('no ivr or music wave')
        
        
        spectrogram   = graph_spectrogram( audio_dir + '/'+ f )
        graygram      = rgb2gray(spectrogram)
        
        shape         = (34,50)
        reshape       = resize(graygram,shape)
        normgram      = normalize_gray(reshape)
        
        
        
        
        redgram       = block_reduce(normgram, block_size =(1,1) , func = np.mean)
        
        x_test[i,:,:] = redgram
       # if len(test_list)-5 == i:
        #    img=Image.fromarray(redgram,'RGB')
         #   img.save(r'C:\Users\sarathkumar.h\Desktop\mp33\afr.png')
            
        del spectrogram 
        del graygram
        del shape
        del reshape
        del normgram 
        del redgram
        print("Progress Test Data: {:2.1%}".format(float(i) / len(test_list)), end="\r")
        
    return x_test, y_test


pathhs=[pat[2]]
inr=0
inm=0
for l in range(len(pathhs)):
    number_of_image=0
    name_ff= [f for f in listdir(pathhs[l])] 
    for o in name_ff:
        frame,times=hh(0,1000,pathhs[0]+'/'+o)
        if 'i' in  o:
            for hhi in range(len(frame)-1):
                t_indexi=inr+hhi
                sit=str(t_indexi)
                s_ci=frame[hhi].set_channels(1)
                s_ci.export(pat[4] + '/' + 'other'+sit,format='wav')
            inr=int(len(frame))+inr
        elif 'r' in o:
            for hhr in range(len(frame)-1):
                t_indexr=inr+hhr
                srt=str(t_indexr)
                s_cr=frame[hhr].set_channels(1)
                s_cr.export(pat[4]+'/'+'other'+srt,format='wav')
            inr=int(len(frame))+inr
        elif 'cli' in o:
            for hhcli in range(len(frame)-1):
                t_indexcli=inr+hhcli
                sclit=str(t_indexcli)
                s_ccli=frame[hhcli].set_channels(1)
                s_ccli.export(pat[4]+'/'+'other'+sclit,format='wav')
            inr=int(len(frame))+inr
        elif 'm' in o:
            for hhm in range(len(frame)-1):
                t_indexm=hhm+inm
                smt=str(t_indexm)
                s_cm=frame[hhm].set_channels(1)
                s_cm.export(pat[4] + '/' + 'music'+smt,format='wav')
            inm=int(len(frame))+inm
            
        else:
            print('No rep,music,client,ivr file')


pathhhs=[pat[3]]
inrr=0
inmm=0
for l in range(len(pathhhs)):
    number_of_image=0
    name_ff= [f for f in listdir(pathhhs[l])] 
    #print('over_all_file_in_path',len(name_ff))
    for o in name_ff:
        frame,times=hh(0,1000,pathhhs[l]+'/'+o)
        if 'r' in  o:
            for hhrr in range(len(frame)-1):
                t_indexrr=hhrr+inrr
                srrt=str(t_indexrr)
                s_crr=frame[hhrr].set_channels(1)
                s_crr.export(pat[5] + '/' + 'other'+srrt,format='wav')
            inrr=len(frame)+inrr
            
        elif 'm' in o:
            for hhmm in range(len(frame)-1):
                t_indexmm=hhmm+inmm
                smmt=str(t_indexmm)
                s_cmm=frame[hhmm].set_channels(1)
                s_cmm.export(pat[5] + '/' + 'music'+smmt,format='wav')
            inmm=len(frame)+inmm
            
        elif 'i' in o:
            for hhii in range(len(frame)-1):
                t_indexii=hhii+inrr
                siit=str(t_indexii)
                s_cii=frame[hhii].set_channels(1)
                s_cii.export(pat[5] + '/' + 'other'+siit,format='wav')
            inrr=len(frame)+inrr 
            
        elif 'cli' in o:
            for hhccli in range(len(frame)-1):
                t_indexccli=hhccli+inrr
                scclit=str(t_indexccli)
                s_cccli=frame[hhccli].set_channels(1)
                s_cccli.export(pat[5] + '/' + 'other'+scclit,format='wav')
            inrr=len(frame)+inrr
        else:
            print('No rep,client,ivr, music file')




def train_model_and_saving_it():
    x_train,y_train=create_train(pat[4])
    x_test,y_test=create_test(pat[5])
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)
    x_train = x_train.reshape(x_train.shape[0], imheight, imwidth, 1)
    x_test = x_test.reshape(x_test.shape[0], imheight, imwidth, 1)
    input_shape = (imheight, imwidth, 1)
    batch_size = 4
    #epochs =1
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    
    model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.adam(), metrics=['accuracy'])
    model.fit(x_train, y_train, batch_size=4, epochs=10, verbose=1, validation_data=(x_test, y_test))
    return model
    
model=train_model_and_saving_it()
model.save(r'C:\Users\sarathkumar.h\Desktop\modeleert.h5')
